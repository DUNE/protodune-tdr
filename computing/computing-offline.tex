\section{Prompt Processing for Data Quality Monitoring}
\label{sec:prompt_processing}

As described in Section~\ref{sec:daq_online_monitoring}, the first
point at which data quality monitoring occurs is directly inside the
DAQ Online Monitoring (OM).  The DAQ computing cluster hardware is
relatively high performance and has access to the full, high-rate data
stream.  As such it is ideal for monitoring algorithms which require
small amounts of CPU and a large fraction of the data.

On the other end of this spectrum, some monitoring algorithms have
large CPU requirements but produce meaningful feedback on relatively
little data.  Running these algorithms on commodity cluster hardware
is more cost effective.  To manage these jobs, a special purpose system
called the ``protoDUNE prompt processing system'' (p3s) is developed.
Unlike traditional batch systems it limits maximum latency to provide
results at the cost of 100\% data throughput.  The p3s is portable to many 
native batch systems and is fully
  user-configurable.  It can run multiple independent sets of multiple
  interdependent jobs, and run them in parallel -- to the
  extent allowed by available CPU resources and while satisfying dependency information.

The prompt processing is expected to sample about 1\% of the most
immediate data just after it is saved by the DAQ and is available to the
hardware on which it runs.  An initial estimate finds that at least
300 dedicated cores will be required to achieve this.  This estimate
must be refined as a comprehensive list of monitoring algorithms is
developed.

The prompt jobs are developed in the familiar form of offline software
modules to LArSoft (Section~\ref{sec:comp:larsoft}).  The processing is expected to include algorithms
from signal processing through to full reconstruction.  Due to the
sharing of the underlying art framework in both prompt processing jobs and
DAQ OM modules, it will be easy to migrate algorithms between the two
contexts in the case of computer-hardware resource constraints.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Production processing}
\label{sec:protodune-offline}

The second major user of the raw data is the production processing.
It will make several passes of 100\% of the raw data over time as
algorithms improve.  It consists principally of 
event reconstruction, which feeds into user analysis, and 
it may involve a data-reduction step prior to reconstruction.
A data-reduction scheme has been developed; its implementation in the data production
processing chain is contingent on having available resources.


Starting with the signal-ROI there are two basic approaches to
reconstruction, which are described in the following
sections.  The first starts with fitting multiple Gaussian
distributions to the waveforms and the second to retaining their
binned structure.

