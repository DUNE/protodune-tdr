\section{Offline Processing of the Experimental and MC Data}
\label{sec:protodune-offline}
%\subsection{Data processing}
%\label{sec:protodune-dataprocess}

The raw data collected by \pd will undergo offline processing. More broadly, the offline data
can be classified as follows:
\begin{itemize}

\item A variety of calibration data derived from experimental data (also including dedicated calibration
runs where necessary, and/or subsamples of data collected specifically for calibration
purposes during normal running conditions)

\item Processed experimental data, which may exist in several parallel branches corresponding to
different reconstruction algorithms being applied, with the purpose of evaluating the performance
of the different algorithms

\item Monte Carlo (MC) data, which will contain multiple event samples to cover various event types
and other conditions during the measurements with protoDUNE

\item Data derived from MC events, and produced with a variety of tracking and pattern recognition algorithms
in order to create a basis for the detector characterization

\end{itemize}

\noindent The production processing will likely have more than one processing step,
thus multiplying the data volume. It is desirable that the derived data would be reduced
in size with respect to the raw data in order to keep the data volume manageable.
It is thus assumed that the size of the processed data will likely be smaller than the input (the raw data). 
Given the considerations for the anticipated raw data volume presented above,
the current plan is to provision $\sim$10\,PB of tape storage to keep the processed data. 
For efficient processing, disk storage will be necessary
to stage a considerable portion of both raw data (inputs) and one or a few steps in processing (outputs).

Extrapolating from our previous experience running Monte Carlo for the previous DUNE MC campaigns of comparable scale,
it is estimated that  a few hundred TB of continuously available disk space will be needed. It is expected that protoDUNE will require
a few~PB of storage at Fermilab to ensure optimal data availability and  processing efficiency. 

\subsection{Prompt Processing}
\label{sec:prompt_processing}
In the present context, \textit{Prompt Processing} means a number of fast signal processing and reconstruction processes
(also called ``express stream'' sometimes) which process a fraction of raw data. Its main purpose is to aid in QA of the data
and produce quick calibrations which may be necessary for high quality monitoring of the detector. As one example,
calculating frequency spectra of noise and watching its level, evolution and other characteristincs is an important aspect of ensuring
the stability of the readout chain.

It is understood
that a limited number of metrics will be calculated to make the proccess as quick as possible in order to enable
the operators to take action should the QA process indicate a potential problem in the detector or the data.

The following parameters of prompt processing need to considered:
\begin{itemize}
\item Desired turnaround time.
\item Location of the computing resources utilized.
\item Fraction and type of data to be processed in this manner.
\end{itemize}

\noindent As to the latter item, it is estimated that processing roughly 1\% of the data stream collected by the detector will
be enough to meet most important goals of prompt processing. 
The general strategy is to locate some of the prompt processing capability at CERN, at a scale adequate for the mid-range
data taking scenario such as presented in Table\, \ref{tab:goldi}. A very preliminary estimate indicated that about 300 cores
will be needed to cope with processing at that rate.

If the need materializes to scale up from this capacity due to necessity to increase the trigger rate,
the plan is to take advantage of the high bandwidth of the network connection between
CERN and a few significant sites in the US (such as FNAL, BNL and others) and do additional processing there.



\subsection{Workload Management}
\label{sec:dune-wms}
% For DUNE requirements to its Workload Management System please see the Computing Model document\,\cite{dune_computing_model}.
At the time of writing, formal requirements are being formulated for the protoDUNE Workload Management System based on
discussions among FNAL and other National Labs, CERN, universities and other stakeholders. There are a few systems currently used by
HEP and IF experiments which somewhat differ in the following aspects:
\begin{itemize}

\item Degree of integration with their corresponding data management system.

\item Depth and breadth of monitoring capabilities offered by the system to the user and to the expeiment's operations team.

\item Ability to handle complex workflows automatically.

\item Functionality to correct various error conditions and failure modes, for example ``retry'' a set of jobs that were stuck in an unfinished state
due to corrupted or otherwise missing data and other such conditions.

\end{itemize}

\noindent
A few existing frameworks are being evaluated in order to identify a solution which works best in the context of \pd.
A few examples of widely deployed WMS include:
\begin{itemize}

\item Dirac (Distributed Infrastructure with Remote Agent Control) WMS~\cite{dirac_wms} used at CERN by LHCb.

\item PanDA (``Production and Distributed Analysis'') \cite{panda_chep13}, which forms the backbone of ATLAS processing and has been used in other experiments such as AMS.
PanDA has scaled up to manage O(10$^6$) Grid jobs daily, upgraded to use a fine-grained workflow management system, added an event service to
make it easier to use short-term opportunistics resource and a host of other features. A distinguishing feature of PanDA is sophisticated and powerful
monitoring system which allows efficient troubleshooting and optimal degree of control over execution of workflows.

\item GlideinWMS \cite{glideinwms_chep13} used in CMS and a variety of projects in other disciplines via the Open Science Grid Consortium.

\item Alien \cite{alien} deployed for the Alice experiment at the LHC.

\end{itemize}

\noindent
FNAL has successfully deployed a Grid toolkit named \textit{jobsub} \cite{jobsub_chep13}, which has been used for a number of years to facilitate users' access to
Grid facilities at FNAL and at other grid sites, such as those participating in the Open Science Grid Consortium ({\tt https://www.opensciencegrid.org/}).
It integrates FNAL-local access via HTCondor facilities with GlideinWMS extensions to reach available resources elsewhere. This tool is a part of
the ``FIFE'' framework which serves the computing needs of the IF experiments at FNAL. Where needed, \textit{jobsub} leverages the CVMFS
network file system for software provisioning and the \textit{ifdh} data handling tool which integrates
with SAM, dCache, Amason S3 etc.

Technology downselect for the \pd Workload Management System will happen after additional consultations and R\&D.


%Implementing a WMS in \pd is desirable since it could be and important factor in ensuring efficient, reliable and flexible prompt processing capability (\ref{sec:prompt_processing}).
