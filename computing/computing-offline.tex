\section{Offline Processing of the Experimental and MC Data}
\label{sec:protodune-offline}



The raw data collected by \pd will undergo offline processing. More broadly, the offline data
can be classified as follows:
\begin{itemize}

\item A variety of calibration data derived from experimental data (also including dedicated calibration
runs where necessary, and/or subsamples of data collected specifically for calibration
purposes during normal running conditions)

\item Processed experimental data, which may exist in several parallel branches corresponding to
different reconstruction algorithms being applied, with the purpose of evaluating the performance
of the different algorithms

\item Monte Carlo (MC) data, which will contain multiple event samples to cover various event types
and other conditions during the measurements with protoDUNE

\item Data derived from MC events, and produced with a variety of tracking and pattern recognition algorithms
in order to create a basis for the detector characterization

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Production processing}
\label{sec:prod-process}

The production processing will likely have more than one processing step,
thus multiplying the data volume. It is desirable that the derived data would be reduced
in size with respect to the raw data in order to keep the data volume manageable.
It is thus assumed that the size of the processed data will likely be smaller than the input (the raw data). 
Given the considerations for the anticipated raw data volume presented above,
the current plan is to provision $\sim$10\,PB of tape storage to keep the processed data. 
For efficient processing, disk storage will be necessary
to stage a considerable portion of both raw data (inputs) and a few instances of processed data (outputs).

Extrapolating from experience running Monte Carlo for previous DUNE MC campaigns,
it is estimated that  a few hundred TB of continuously available disk space will be needed. Overall it is expected that \pdsp will require
a few~PB of storage at Fermilab to ensure optimal data availability and  processing efficiency. 
\fixme{Need better justification of these estimates, e.g. add quantitative estimates for components in above bullet list - these should add up to the claimed few PB.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Prompt Processing}
\label{sec:prompt_processing}

In the present context, \textit{Prompt Processing} means a number of fast signal processing and reconstruction processes
(also called ``express stream'' sometimes) which process a fraction of raw data. Its main purpose is to aid in QA of the data
and produce quick calibrations which may be necessary for high quality monitoring of the detector. As one example,
calculating frequency spectra of noise and watching its level, evolution and other characteristics is an important aspect of ensuring
the stability of the readout chain.

It is understood
that a limited number of metrics will be calculated to make the proccess as quick as possible in order to enable
the operators to take action should the QA process indicate a potential problem in the detector or the data.

It is estimated that processing roughly 1\% of the data stream collected by the detector will
be enough to meet most important goals of prompt processing. 
The general strategy is to locate some of the prompt processing capability at CERN, at a scale adequate for the mid-range
data taking scenario such as presented in Table\, \ref{tab:goldi}. An estimated 300 cores
will be needed to cope with processing at that rate.

