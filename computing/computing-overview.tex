\section{Overview}

This chapter outlines the technical design of the offline computing system and introduces the software
that is used to simulate and reconstruct data.

The data rate and the total data volume are the main factors influencing the design choices and scale of the offline computing system.   The system provides resources necessary for data distribution, processing, and analysis on the grid\cite{data_managm_sys}.  

The planned data processing steps include  
calibration, reconstruction, ntuplizing and user analysis.
Multiple passes through this chain will be required for final results as
calibrations, algorithms, and ntuple definitions are expected to evolve.

A fraction of the data will be subject to \textit{prompt processing}, which performs partial reconstruction of the data for QA/QC purposes
with a short turnaround time (see\,\ref{sec:prompt_processing}). 
 A metadata and file catalog system is necessary for managing the data.
